{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Imports \n",
    "import numpy as np #Represent ndarrays a.k.a. tensors\n",
    "import matplotlib.pyplot as plt #For plotting\n",
    "np.random.seed(0) #For repeatability of the experiment\n",
    "import pickle #To read data for this experiment\n",
    "import _pickle as cPickle\n",
    "import gzip\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "#Setup\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read data\n",
    "def load_data():\n",
    "    with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "        training_data, validation_data, test_data = cPickle.load(f, encoding=\"latin-1\")\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = np.array(list(zip(training_inputs, training_results)))\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = np.array(list(zip(validation_inputs, va_d[1])))\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = np.array(list(zip(test_inputs, te_d[1])))\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "training_data, validation_data, test_data = load_data_wrapper()\n",
    "\n",
    "training_data_nolab = np.delete(training_data,[1],axis = 1)\n",
    "training_labels = np.array(np.delete(training_data,[0],axis=1))\n",
    "training_labels_list = []\n",
    "training_data_nolab_list = np.zeros(shape=(50000,784))\n",
    "\n",
    "for i in range(len(training_labels)):\n",
    "    list.append(training_labels_list, np.argmax(training_labels[i,0]))\n",
    "\n",
    "for i in range(len(training_data_nolab)):\n",
    "    training_data_nolab_list[i] = np.array(np.ravel(training_data_nolab[i,0]))\n",
    "\n",
    "X = training_data_nolab_list\n",
    "y = np.squeeze(training_labels_list)\n",
    "\n",
    "test_data_nolab = np.delete(test_data,[1],axis = 1)\n",
    "test_labels = np.array(np.delete(test_data,[0],axis=1))\n",
    "test_labels_list = []\n",
    "test_data_nolab_list = np.zeros(shape=(10000,784))\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    list.append(test_labels_list, np.argmax(test_labels[i,0]))\n",
    "\n",
    "for i in range(len(test_data_nolab)):\n",
    "    test_data_nolab_list[i] = np.array(np.ravel(test_data_nolab[i,0]))\n",
    "\n",
    "X_test = test_data_nolab_list\n",
    "y_test = np.squeeze(test_labels_list)\n",
    "\n",
    "valid_data_nolab = np.delete(validation_data,[1],axis = 1)\n",
    "valid_labels = np.array(np.delete(validation_data,[0],axis=1))\n",
    "valid_labels_list = []\n",
    "valid_data_nolab_list = np.zeros(shape=(10000,784))\n",
    "\n",
    "for i in range(len(valid_labels)):\n",
    "    list.append(valid_labels_list, np.argmax(valid_labels[i,0]))\n",
    "\n",
    "for i in range(len(valid_data_nolab)):\n",
    "    valid_data_nolab_list[i] = np.array(np.ravel(valid_data_nolab[i,0]))\n",
    "\n",
    "X_valid = valid_data_nolab_list\n",
    "y_valid = np.squeeze(valid_labels_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization = 0.4\n",
      "[0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1]\n"
     ]
    }
   ],
   "source": [
    "# Feedforward neural net model\n",
    "\n",
    "# Start with an initial set of parameters randomly\n",
    "\n",
    "D = X.shape[1] #Number of features\n",
    "K = 10 #Number of classes assuming class index starts from 0\n",
    "\n",
    "h = 100 # size of hidden layer\n",
    "W3 = 0.01*np.random.randn(h,K)  ## hidden layers *classes\n",
    "b3 = np.zeros((1,K)) \n",
    "W1 = 0.01*np.random.randn(D,h)\n",
    "b1 = np.zeros((1,h))\n",
    "W2 = 0.01*np.random.randn(D,h)\n",
    "b2 = np.zeros((1,h))\n",
    "\n",
    "# Initial values from hyperparameter\n",
    "reg = 4e-1 # regularization strength\n",
    "print(\"regularization =\",reg)\n",
    "#For simplicity, we will not optimize this using grid search here.\n",
    "\n",
    "#optimizing the reg parameter\n",
    "reg_values = list([0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1])\n",
    "print(reg_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "reg param: 0.4\n",
      "step size: 0.8\n",
      "iteration: 0  loss: 5.42442584316\n",
      "iteration: 2  loss: 2.962932248\n",
      "iteration: 4  loss: 2.43855725574\n",
      "iteration: 6  loss: 2.32517450949\n",
      "iteration: 8  loss: 2.29888512383\n",
      "iteration: 10  loss: 2.29068734334\n",
      "iteration: 12  loss: 2.28545096463\n",
      "iteration: 14  loss: 2.27937436819\n",
      "iteration: 16  loss: 2.27249837006\n",
      "iteration: 18  loss: 2.26464712868\n",
      "iteration: 20  loss: 2.25596765796\n",
      "iteration: 22  loss: 2.24771979026\n",
      "iteration: 24  loss: 2.23959234215\n",
      "iteration: 26  loss: 2.23237204175\n",
      "iteration: 28  loss: 2.22528568641\n",
      "iteration: 30  loss: 2.219117408\n",
      "iteration: 32  loss: 2.2129323268\n",
      "iteration: 34  loss: 2.20732812983\n",
      "iteration: 36  loss: 2.20234330525\n",
      "iteration: 38  loss: 2.19680879685\n",
      "iteration: 40  loss: 2.19194074256\n",
      "iteration: 42  loss: 2.18781976672\n",
      "iteration: 44  loss: 2.1843884273\n",
      "iteration: 46  loss: 2.18188737861\n",
      "iteration: 48  loss: 2.17870315985\n",
      "test accuracy: 0.2905\n"
     ]
    }
   ],
   "source": [
    "#Perform batch SGD using manual backprop\n",
    "\n",
    "#For simplicity we will take the batch size to be the same as number of examples\n",
    "num_examples = X.shape[0]\n",
    "print(num_examples)\n",
    "\n",
    "#Initial value for the Gradient Descent Parameter\n",
    "step_size = 0.8e-0 #Also called learning rate\n",
    "#For simplicity, we will not hand tune this algorithm parameter as well.\n",
    "\n",
    "print(\"reg param:\",reg)\n",
    "print(\"step size:\",step_size)\n",
    "#print(\"iterations:\",gd_iters)\n",
    "\n",
    "# gradient descent loop\n",
    "for i in range(50):\n",
    "    # evaluate class scores, [N x K]\n",
    "    hidden_layer1 = np.dot(X, W1) + b1\n",
    "    hidden_layer2 = np.dot(X, W2) + b2\n",
    "    hidden_layer = np.maximum(hidden_layer1, hidden_layer2)\n",
    "    scores = np.dot(hidden_layer, W3) + b3\n",
    "    \n",
    "    # compute the class probabilities\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "    \n",
    "    # compute the loss: average cross-entropy loss and regularization\n",
    "    corect_logprobs = -np.log(probs[range(num_examples),y])\n",
    "    data_loss = np.sum(corect_logprobs)/num_examples\n",
    "    reg_loss = 0.5*reg*np.sum(W1*W1) + 0.5*reg*np.sum(W2*W2)\n",
    "    loss = data_loss + reg_loss\n",
    "    if i % 2 == 0:\n",
    "        print (\"iteration:\",i, \" loss:\", loss)\n",
    "\n",
    "    # compute the gradient on scores\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),y] -= 1\n",
    "    dscores /= num_examples\n",
    "        \n",
    "    # backpropate the gradient to the parameters\n",
    "    \n",
    "    # backpropate the gradient to the parameters\n",
    "    dW3 = np.dot(hidden_layer.T, dscores)\n",
    "    db3 = np.sum(dscores, axis = 0, keepdims = True)\n",
    "        \n",
    "    # first backprop into parameters W2 and b2\n",
    "    dW2 = np.dot(hidden_layer.T, dscores)\n",
    "    db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "    \n",
    "    # next backprop into hidden layer\n",
    "    dhidden1 = np.dot(dscores, W3.T)\n",
    "    dhidden1[hidden_layer1 <= hidden_layer2] = 0\n",
    "    dhidden2 = np.dot(dscores, W3.T)\n",
    "    dhidden2[hidden_layer2 <= hidden_layer1] = 0\n",
    "\n",
    "    ## final backprop\n",
    "    dW1 = np.dot(X.T, dhidden1)\n",
    "    db1 = np.sum(dhidden1, axis = 0, keepdims = True)\n",
    "\n",
    "    dW2 = np.dot(X.T, dhidden2)\n",
    "    db2 = np.sum(dhidden2, axis = 0, keepdims = True)\n",
    "    ## adding reg to gradient\n",
    "    dW2 += reg*W2\n",
    "    dW1 += reg*W1\n",
    "    dW3 += reg*W3\n",
    "    \n",
    "    ## stepsize\n",
    "    W1 += -step_size * dW1\n",
    "    W2 += -step_size * dW2\n",
    "    W3 += -step_size * dW3\n",
    "\n",
    "    b2 += -step_size * db2\n",
    "    b1 += -step_size * db1\n",
    "    b3 += -step_size * db3\n",
    "\n",
    "# Post-training: evaluate model accuracy\n",
    "hidden_layer = np.maximum(np.dot(X_valid, W1) + b1, np.dot(X_valid, W2) + b2) \n",
    "scores = np.dot(hidden_layer, W3) + b3\n",
    "predicted_class = np.argmax(scores, axis = 1) ### scores with max conf\n",
    "test_accuracy = (np.mean(predicted_class == y_valid))\n",
    "print(\"test accuracy:\",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Post Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.2962\n"
     ]
    }
   ],
   "source": [
    "# Post-training: evaluate model accuracy\n",
    "hidden_layer = np.maximum(np.dot(X_test, W1) + b1, np.dot(X_test, W2) + b2) \n",
    "scores = np.dot(hidden_layer, W3) + b3\n",
    "predicted_class = np.argmax(scores, axis = 1) ### scores with max conf\n",
    "test_accuracy = (np.mean(predicted_class == y_test))\n",
    "print(\"test accuracy:\",test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
