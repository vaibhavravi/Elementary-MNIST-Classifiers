{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Imports \n",
    "import numpy as np #Represent ndarrays a.k.a. tensors\n",
    "import matplotlib.pyplot as plt #For plotting\n",
    "np.random.seed(0) #For repeatability of the experiment\n",
    "import pickle #To read data for this experiment\n",
    "import _pickle as cPickle\n",
    "import gzip\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "#Setup\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read data\n",
    "def load_data():\n",
    "    with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "        training_data, validation_data, test_data = cPickle.load(f, encoding=\"latin-1\")\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = np.array(list(zip(training_inputs, training_results)))\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = np.array(list(zip(validation_inputs, va_d[1])))\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = np.array(list(zip(test_inputs, te_d[1])))\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "training_data, validation_data, test_data = load_data_wrapper()\n",
    "\n",
    "training_data_nolab = np.delete(training_data,[1],axis = 1)\n",
    "training_labels = np.array(np.delete(training_data,[0],axis=1))\n",
    "training_labels_list = []\n",
    "training_data_nolab_list = np.zeros(shape=(50000,784))\n",
    "\n",
    "for i in range(len(training_labels)):\n",
    "    list.append(training_labels_list, np.argmax(training_labels[i,0]))\n",
    "\n",
    "for i in range(len(training_data_nolab)):\n",
    "    training_data_nolab_list[i] = np.array(np.ravel(training_data_nolab[i,0]))\n",
    "\n",
    "X = training_data_nolab_list\n",
    "y = np.squeeze(training_labels_list)\n",
    "\n",
    "test_data_nolab = np.delete(test_data,[1],axis = 1)\n",
    "test_labels = np.array(np.delete(test_data,[0],axis=1))\n",
    "test_labels_list = []\n",
    "test_data_nolab_list = np.zeros(shape=(10000,784))\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    list.append(test_labels_list, np.argmax(test_labels[i,0]))\n",
    "\n",
    "for i in range(len(test_data_nolab)):\n",
    "    test_data_nolab_list[i] = np.array(np.ravel(test_data_nolab[i,0]))\n",
    "\n",
    "X_test = test_data_nolab_list\n",
    "y_test = np.squeeze(test_labels_list)\n",
    "\n",
    "valid_data_nolab = np.delete(validation_data,[1],axis = 1)\n",
    "valid_labels = np.array(np.delete(validation_data,[0],axis=1))\n",
    "valid_labels_list = []\n",
    "valid_data_nolab_list = np.zeros(shape=(10000,784))\n",
    "\n",
    "for i in range(len(valid_labels)):\n",
    "    list.append(valid_labels_list, np.argmax(valid_labels[i,0]))\n",
    "\n",
    "for i in range(len(valid_data_nolab)):\n",
    "    valid_data_nolab_list[i] = np.array(np.ravel(valid_data_nolab[i,0]))\n",
    "\n",
    "X_valid = valid_data_nolab_list\n",
    "y_valid = np.squeeze(valid_labels_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization = 0.4\n",
      "[0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1]\n"
     ]
    }
   ],
   "source": [
    "# Feedforward neural net model\n",
    "\n",
    "# Start with an initial set of parameters randomly\n",
    "\n",
    "D = X.shape[1] #Number of features\n",
    "K = 10 #Number of classes assuming class index starts from 0\n",
    "\n",
    "h = 100 # size of hidden layer\n",
    "W = 0.01 * np.random.randn(D,h)\n",
    "b = np.zeros((1,h))\n",
    "W2 = 0.01 * np.random.randn(h,K)\n",
    "b2 = np.zeros((1,K))\n",
    "\n",
    "# Initial values from hyperparameter\n",
    "reg = 4e-1 # regularization strength\n",
    "print(\"regularization =\",reg)\n",
    "#For simplicity, we will not optimize this using grid search here.\n",
    "\n",
    "#optimizing the reg parameter\n",
    "reg_values = list([0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1])\n",
    "print(reg_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "reg param: 0.4\n",
      "step size: 0.8\n",
      "iteration: 0  loss: 3.87803261521\n",
      "iteration: 2  loss: 2.63697432479\n",
      "iteration: 4  loss: 2.37269566041\n",
      "iteration: 6  loss: 2.31564989829\n",
      "iteration: 8  loss: 2.3026767213\n",
      "iteration: 10  loss: 2.29892786599\n",
      "iteration: 12  loss: 2.29690994518\n",
      "iteration: 14  loss: 2.29501279598\n",
      "iteration: 16  loss: 2.29295119088\n",
      "iteration: 18  loss: 2.2908127449\n",
      "iteration: 20  loss: 2.28877976781\n",
      "iteration: 22  loss: 2.28702693491\n",
      "iteration: 24  loss: 2.2858718787\n",
      "iteration: 26  loss: 2.28566593166\n",
      "iteration: 28  loss: 2.28523800365\n",
      "iteration: 30  loss: 2.28475162655\n",
      "iteration: 32  loss: 2.28418500008\n",
      "iteration: 34  loss: 2.28322035603\n",
      "iteration: 36  loss: 2.28229514209\n",
      "iteration: 38  loss: 2.28144705506\n",
      "iteration: 40  loss: 2.28071112497\n",
      "iteration: 42  loss: 2.28000317029\n",
      "iteration: 44  loss: 2.27936090466\n",
      "iteration: 46  loss: 2.27874962366\n",
      "iteration: 48  loss: 2.2782095759\n",
      "iteration: 50  loss: 2.27771622123\n",
      "iteration: 52  loss: 2.277261604\n",
      "iteration: 54  loss: 2.27685665771\n",
      "iteration: 56  loss: 2.27650006746\n",
      "iteration: 58  loss: 2.2761665068\n",
      "iteration: 60  loss: 2.27584623364\n",
      "iteration: 62  loss: 2.27552643651\n",
      "iteration: 64  loss: 2.2752169226\n",
      "iteration: 66  loss: 2.27491065274\n",
      "iteration: 68  loss: 2.27461827731\n",
      "iteration: 70  loss: 2.27432827084\n",
      "iteration: 72  loss: 2.27405731507\n",
      "iteration: 74  loss: 2.27379640817\n",
      "iteration: 76  loss: 2.27354533914\n",
      "iteration: 78  loss: 2.27329461651\n",
      "iteration: 80  loss: 2.27303302268\n",
      "iteration: 82  loss: 2.272777114\n",
      "iteration: 84  loss: 2.27253192057\n",
      "iteration: 86  loss: 2.27229440899\n",
      "iteration: 88  loss: 2.27206010365\n",
      "iteration: 90  loss: 2.27182324896\n",
      "iteration: 92  loss: 2.27159024377\n",
      "iteration: 94  loss: 2.27135571234\n",
      "iteration: 96  loss: 2.27112562041\n",
      "iteration: 98  loss: 2.27090459184\n",
      "accuracy:  0.4186\n"
     ]
    }
   ],
   "source": [
    "#Perform batch SGD using manual backprop\n",
    "\n",
    "#For simplicity we will take the batch size to be the same as number of examples\n",
    "num_examples = X.shape[0]\n",
    "print(num_examples)\n",
    "\n",
    "#Initial value for the Gradient Descent Parameter\n",
    "step_size = 0.8e-0 #Also called learning rate\n",
    "#For simplicity, we will not hand tune this algorithm parameter as well.\n",
    "\n",
    "print(\"reg param:\",reg)\n",
    "print(\"step size:\",step_size)\n",
    "#print(\"iterations:\",gd_iters)\n",
    "\n",
    "# gradient descent loop\n",
    "for i in range(100):\n",
    "\n",
    "    # evaluate class scores, [N x K]\n",
    "    hidden_layer = np.maximum(np.dot(X, W) + b, 0.01*(np.dot(X, W) + b))\n",
    "    scores = np.dot(hidden_layer, W2) + b2\n",
    "\n",
    "    # compute the class probabilities\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "    \n",
    "    # compute the loss: average cross-entropy loss and regularization\n",
    "    corect_logprobs = -np.log(probs[range(num_examples),y])\n",
    "    data_loss = np.sum(corect_logprobs)/num_examples\n",
    "    reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n",
    "    loss = data_loss + reg_loss\n",
    "    if i % 2 == 0:\n",
    "        print (\"iteration:\",i, \" loss:\", loss)\n",
    "\n",
    "    # compute the gradient on scores\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),y] -= 1\n",
    "    dscores /= num_examples\n",
    "        \n",
    "    # backpropate the gradient to the parameters\n",
    "    # first backprop into parameters W2 and b2\n",
    "    dW2 = np.dot(hidden_layer.T, dscores)\n",
    "    db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "    \n",
    "    # next backprop into hidden layer\n",
    "    dhidden = np.dot(dscores, W2.T)\n",
    "    var = 0.01*np.dot(dscores, W2.T)\n",
    "    \n",
    "    dhidden[hidden_layer <= 0] = 0.01*dhidden[hidden_layer <= 0]  \n",
    "\n",
    "    ## final backprop\n",
    "    dW = np.dot(X.T, dhidden)\n",
    "    db = np.sum(dhidden, axis = 0, keepdims = True)\n",
    "\n",
    "    ## adding reg to gradient\n",
    "    dW2 += reg*W2\n",
    "    dW += reg*W\n",
    "\n",
    "    ## stepsize\n",
    "    W += -step_size * dW\n",
    "    W2 += -step_size * dW2\n",
    "    b2 += -step_size * db2\n",
    "    b += -step_size * db\n",
    "\n",
    "\n",
    "#def test_ffn_leakyrelu_classifier(X_data, y_data, W, b, W2, b2):\n",
    "# Post-training: evaluate model accuracy\n",
    "hidden_layer = np.maximum(0.01*np.dot(X_valid, W) + b, np.dot(X_valid, W) + b) \n",
    "scores = np.dot(hidden_layer, W2) + b2\n",
    "predicted_class = np.argmax(scores, axis = 1) ### scores with max conf\n",
    "test_accuracy = (np.mean(predicted_class == y_valid))\n",
    "print(\"accuracy: \",test_accuracy)\n",
    "#return test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Post Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.4234\n"
     ]
    }
   ],
   "source": [
    "#def test_ffn_leakyrelu_classifier(X_data, y_data, W, b, W2, b2):\n",
    "# Post-training: evaluate model accuracy\n",
    "hidden_layer = np.maximum(0.01*np.dot(X_test, W) + b, np.dot(X_test, W) + b) \n",
    "scores = np.dot(hidden_layer, W2) + b2\n",
    "predicted_class = np.argmax(scores, axis = 1) ### scores with max conf\n",
    "test_accuracy = (np.mean(predicted_class == y_test))\n",
    "print(\"accuracy: \",test_accuracy)\n",
    "#return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
