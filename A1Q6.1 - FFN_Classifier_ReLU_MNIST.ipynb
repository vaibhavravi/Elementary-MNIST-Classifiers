{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Imports \n",
    "import numpy as np #Represent ndarrays a.k.a. tensors\n",
    "import matplotlib.pyplot as plt #For plotting\n",
    "np.random.seed(0) #For repeatability of the experiment\n",
    "import pickle #To read data for this experiment\n",
    "import _pickle as cPickle\n",
    "import gzip\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "#Setup\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read data\n",
    "def load_data():\n",
    "    with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "        training_data, validation_data, test_data = cPickle.load(f, encoding=\"latin-1\")\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = np.array(list(zip(training_inputs, training_results)))\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = np.array(list(zip(validation_inputs, va_d[1])))\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = np.array(list(zip(test_inputs, te_d[1])))\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "training_data, validation_data, test_data = load_data_wrapper()\n",
    "\n",
    "training_data_nolab = np.delete(training_data,[1],axis = 1)\n",
    "training_labels = np.array(np.delete(training_data,[0],axis=1))\n",
    "training_labels_list = []\n",
    "training_data_nolab_list = np.zeros(shape=(50000,784))\n",
    "\n",
    "for i in range(len(training_labels)):\n",
    "    list.append(training_labels_list, np.argmax(training_labels[i,0]))\n",
    "\n",
    "for i in range(len(training_data_nolab)):\n",
    "    training_data_nolab_list[i] = np.array(np.ravel(training_data_nolab[i,0]))\n",
    "\n",
    "X = training_data_nolab_list\n",
    "y = np.squeeze(training_labels_list)\n",
    "\n",
    "test_data_nolab = np.delete(test_data,[1],axis = 1)\n",
    "test_labels = np.array(np.delete(test_data,[0],axis=1))\n",
    "test_labels_list = []\n",
    "test_data_nolab_list = np.zeros(shape=(10000,784))\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    list.append(test_labels_list, np.argmax(test_labels[i,0]))\n",
    "\n",
    "for i in range(len(test_data_nolab)):\n",
    "    test_data_nolab_list[i] = np.array(np.ravel(test_data_nolab[i,0]))\n",
    "\n",
    "X_test = test_data_nolab_list\n",
    "y_test = np.squeeze(test_labels_list)\n",
    "\n",
    "valid_data_nolab = np.delete(validation_data,[1],axis = 1)\n",
    "valid_labels = np.array(np.delete(validation_data,[0],axis=1))\n",
    "valid_labels_list = []\n",
    "valid_data_nolab_list = np.zeros(shape=(10000,784))\n",
    "\n",
    "for i in range(len(valid_labels)):\n",
    "    list.append(valid_labels_list, np.argmax(valid_labels[i,0]))\n",
    "\n",
    "for i in range(len(valid_data_nolab)):\n",
    "    valid_data_nolab_list[i] = np.array(np.ravel(valid_data_nolab[i,0]))\n",
    "\n",
    "X_valid = valid_data_nolab_list\n",
    "y_valid = np.squeeze(valid_labels_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feedforward neural net model\n",
    "\n",
    "# Start with an initial set of parameters randomly\n",
    "\n",
    "D = X.shape[1] #Number of features\n",
    "K = 10 #Number of classes assuming class index starts from 0\n",
    "\n",
    "h = 100 # size of hidden layer\n",
    "W = 0.01 * np.random.randn(D,h)\n",
    "b = np.zeros((1,h))\n",
    "W2 = 0.01 * np.random.randn(h,K)\n",
    "b2 = np.zeros((1,K))\n",
    "\n",
    "# Initial values from hyperparameter\n",
    "reg = 4e-1 # regularization strength\n",
    "print(\"regularization =\",reg)\n",
    "#For simplicity, we will not optimize this using grid search here.\n",
    "\n",
    "#optimizing the reg parameter\n",
    "reg_values = list([0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1])\n",
    "print(reg_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Perform batch SGD using manual backprop\n",
    "\n",
    "#For simplicity we will take the batch size to be the same as number of examples\n",
    "num_examples = X.shape[0]\n",
    "\n",
    "#Initial value for the Gradient Descent Parameter\n",
    "step_size = 1e-0 #Also called learning rate\n",
    "print(\"learning rate = \",step_size)\n",
    "#For simplicity, we will not hand tune this algorithm parameter as well.\n",
    "\n",
    "#for reg in reg_values:\n",
    "    \n",
    "print(\"regularization value:\",reg)\n",
    "# gradient descent loop\n",
    "for i in range(100):\n",
    "\n",
    "  # evaluate class scores, [N x K]\n",
    "  hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation\n",
    "  scores = np.dot(hidden_layer, W2) + b2\n",
    "\n",
    "  # compute the class probabilities\n",
    "  exp_scores = np.exp(scores)\n",
    "  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "  # compute the loss: average cross-entropy loss and regularization\n",
    "  corect_logprobs = -np.log(probs[range(num_examples),y])\n",
    "  data_loss = np.sum(corect_logprobs)/num_examples\n",
    "  reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n",
    "  loss = data_loss + reg_loss\n",
    "  if i % 50 == 0:\n",
    "    print (\"iteration:\",i, \" loss:\", loss)\n",
    "\n",
    "  # compute the gradient on scores\n",
    "  dscores = probs\n",
    "  dscores[range(num_examples),y] -= 1\n",
    "  dscores /= num_examples\n",
    "\n",
    "  # backpropate the gradient to the parameters\n",
    "  # first backprop into parameters W2 and b2\n",
    "  dW2 = np.dot(hidden_layer.T, dscores)\n",
    "  db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "  # next backprop into hidden layer\n",
    "  dhidden = np.dot(dscores, W2.T)\n",
    "  # backprop the ReLU non-linearity\n",
    "  dhidden[hidden_layer <= 0] = 0\n",
    "  # finally into W,b\n",
    "  dW = np.dot(X.T, dhidden)\n",
    "  db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "\n",
    "  # add regularization gradient contribution\n",
    "  dW2 += reg * W2\n",
    "  dW += reg * W\n",
    "\n",
    "  # perform a parameter update\n",
    "  W += -step_size * dW\n",
    "  b += -step_size * db\n",
    "  W2 += -step_size * dW2\n",
    "  b2 += -step_size * db2\n",
    "\n",
    "# Post-training: evaluate validation set accuracy\n",
    "# X_test = X\n",
    "# y_test = y\n",
    "print(\"iteration:\",i, \" loss:\", loss)\n",
    "hidden_layer = np.maximum(0, np.dot(X_valid, W) + b)\n",
    "scores = np.dot(hidden_layer, W2) + b2\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print (\"validation accuracy: \",(np.mean(predicted_class == y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Post Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Post-training: evaluate test set accuracy\n",
    "\n",
    "hidden_layer = np.maximum(0, np.dot(X_test, W) + b)\n",
    "scores = np.dot(hidden_layer, W2) + b2\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print (\"test accuracy: \",(np.mean(predicted_class == y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
