{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports \n",
    "import numpy as np #Represent ndarrays a.k.a. tensors\n",
    "import matplotlib.pyplot as plt #For plotting\n",
    "np.random.seed(0) #For repeatability of the experiment\n",
    "import pickle #To read data for this experiment\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def unpickle(file):\n",
    "    \n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo, encoding=\"latin-1\")\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def load_data():\n",
    "    X = []\n",
    "    y = []\n",
    "    for p in range(5):\n",
    "        data = unpickle('cifar-10-batches-py/data_batch_'+str(p+1))\n",
    "        #print(data.shape)\n",
    "        X_var = data[\"data\"]\n",
    "        y_var = data[\"labels\"]\n",
    "        X.append(X_var)\n",
    "        y.append(y_var)\n",
    "    \n",
    "    X_var = np.concatenate(X)\n",
    "    y_train = np.concatenate(y).astype(np.int32)\n",
    "    \n",
    "    data = unpickle('cifar-10-batches-py/test_batch')\n",
    "    X_test1 = data[\"data\"]\n",
    "    y_test = np.array(data[\"labels\"],dtype = np.int32)\n",
    "    \n",
    "    X_train = np.reshape(X_var,(X_var.shape[0],-1))\n",
    "    X_test = np.reshape(X_test1,(X_test1.shape[0],-1))\n",
    "    \n",
    "    ## Adding bias - tranform shape\n",
    "#     X_train = np.hstack([X_train,np.ones((X_train.shape[0],1))])\n",
    "#     X_test = np.hstack([X_test,np.ones((X_test.shape[0],1))])\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train1, y_train1, X_test1, y_test1 = load_data()\n",
    "\n",
    "Xtr = normalize(X_train1, axis = 1, norm = \"l1\")\n",
    "Xtst = normalize(X_test1, axis = 1, norm = \"l1\")\n",
    "ytst = y_test1\n",
    "ytr = y_train1\n",
    "\n",
    "# -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear model    \n",
    "def train_linear_classifier(X_train, y_train, step_size, reg, gd_iters):    \n",
    "    #Define some local varaibles\n",
    "    D = X_train.shape[1] #Number of features\n",
    "    K = max(y_train)+1 #Number of classes assuming class index starts from 0\n",
    "\n",
    "    # Initialize parameters randomly\n",
    "    W = 0.01 * np.random.randn(D,K)\n",
    "    b = np.zeros((1,K))# Initial values from hyperparameter\n",
    "    \n",
    "    \n",
    "    #Perform batch SGD using backprop\n",
    "\n",
    "    #For simplicity we will take the batch size to be the same as number of examples\n",
    "    num_examples = X_train.shape[0]\n",
    "\n",
    "    print(\"reg param:\",reg)\n",
    "    print(\"step size:\",step_size)\n",
    "    print(\"iterations:\",gd_iters)\n",
    "    \n",
    "    # gradient descent loop\n",
    "    for i in range(gd_iters):\n",
    "\n",
    "        # evaluate class scores, [N x K]\n",
    "        scores = np.dot(X_train, W) + b\n",
    "\n",
    "        # compute the class probabilities\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "        # compute the loss: average cross-entropy loss and regularization\n",
    "        corect_logprobs = -np.log(probs[range(num_examples),y_train])\n",
    "        data_loss = np.sum(corect_logprobs)/num_examples\n",
    "        reg_loss = 0.5*reg*np.sum(W*W)\n",
    "        loss = data_loss + reg_loss\n",
    "#         if i % 100 == 0:\n",
    "#             print(\"iteration:\",i, \" loss:\",loss)\n",
    "\n",
    "        # compute the gradient on scores\n",
    "        dscores = probs\n",
    "        dscores[range(num_examples),y_train] -= 1\n",
    "        dscores /= num_examples\n",
    "\n",
    "        # backpropate the gradient to the parameters (W,b)\n",
    "        dW = np.dot(X_train.T, dscores)\n",
    "        db = np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "        dW += reg*W # regularization gradient\n",
    "\n",
    "        # perform a parameter update\n",
    "        W += -step_size * dW\n",
    "        b += -step_size * db\n",
    "    return W, b\n",
    "\n",
    "def test_linear_classifier(X_data, y_data, W, b):\n",
    "    scores = np.dot(X_data, W) + b\n",
    "    predicted_class = np.argmax(scores, axis=1)\n",
    "    test_accuracy = (np.mean(predicted_class == y_data))\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split data into test and train with 80:20 ratio\n",
    "#Xtr, Xtst, ytr, ytst = test_train_data_split(X,y,0.8)\n",
    "\n",
    "W_tr, b_tr = train_linear_classifier(Xtr, ytr, 0.8, 0.4, 100)\n",
    "\n",
    "# testing the performance on hold out set\n",
    "print(\"Accuracy\")\n",
    "train_accuracy = test_linear_classifier(Xtr, ytr, W_tr, b_tr)\n",
    "print(\"train accuracy: \",train_accuracy,\"\\n\")\n",
    "test_accuracy = test_linear_classifier(Xtst, ytst, W_tr, b_tr)\n",
    "print(\"test accuracy: \",test_accuracy,\"\\n\")\n",
    "#plotting model result\n",
    "#plot_result(Xtst, ytst, W_tr, b_tr)\n",
    "\n",
    "#using several different values for the hyper parameter:\n",
    "step_size_values = list(np.arange(0,100,10))  #Also called learning rate\n",
    "# step_size_values = np.append(step_size_values, list(np.arange(1,10,1)))\n",
    "# step_size_values = np.append(step_size_values, list(np.arange(10,100,5)))\n",
    "\n",
    "test_results = [0] * len(step_size_values)\n",
    "print(\"Changing step size\")\n",
    "for i, step_size in enumerate(step_size_values):\n",
    "    W_tr, b_tr = train_linear_classifier(Xtr, ytr, step_size, 0.4, 100)\n",
    "    test_result = test_linear_classifier(Xtst, ytst, W_tr, b_tr)\n",
    "    print(\"test accuracy: \",test_result,\"\\n\")\n",
    "    test_results[i] = test_result\n",
    "\n",
    "plt.plot(step_size_values,test_results,'-')\n",
    "plt.axis([0, max(step_size_values)+1, 0, 1])\n",
    "plt.ylabel('test accuracy')\n",
    "plt.xlabel('learning rate')\n",
    "plt.show()\n",
    "\n",
    "## changing reg parameter\n",
    "reg_values = list([0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 1])\n",
    "\n",
    "test_results = [0] * len(reg_values)\n",
    "print(\"Changing Reg parameter\")\n",
    "for i, reg_value in enumerate(reg_values):\n",
    "    W_tr, b_tr = train_linear_classifier(Xtr, ytr, 1, reg_value, 100)\n",
    "    test_result = test_linear_classifier(Xtst, ytst, W_tr, b_tr)\n",
    "    print(\"test accuracy: \",test_result,\"\\n\")\n",
    "    test_results[i] = test_result\n",
    "\n",
    "plt.plot(reg_values,test_results,'-')\n",
    "plt.axis([0, max(reg_values), 0, 1])\n",
    "plt.ylabel('test accuracy')\n",
    "plt.xlabel('regularization value')\n",
    "plt.show()\n",
    "\n",
    "# modifying the number of gradient descent iterations\n",
    "# reg = 0.4, step_size = 1\n",
    "gd_iters_vals = list([100, 500, 1000])\n",
    "\n",
    "test_results = [0] * len(gd_iters_vals)\n",
    "print(\"Changing no of iterations\")\n",
    "for i, gd_iters in enumerate(gd_iters_vals):\n",
    "    W_tr, b_tr = train_linear_classifier(Xtr, ytr, 1, 0.4, gd_iters)\n",
    "    test_result = test_linear_classifier(Xtst, ytst, W_tr, b_tr)\n",
    "    print(\"test accuracy: \",test_result,\"\\n\")\n",
    "    test_results[i] = test_result\n",
    "\n",
    "plt.plot(gd_iters_vals,test_results,'-')\n",
    "plt.axis([0, max(gd_iters_vals), 0, 1])\n",
    "plt.ylabel('test accuracy')\n",
    "plt.xlabel('no of iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CIFAR 10 on FFN RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FFN model    \n",
    "def train_ffn_classifier(X_train, y_train, step_size, reg, gd_iters):    \n",
    "    h = 100 ## hidden layers\n",
    "    K = max(y_train)+1   ## no of O/P classes --> considerin class starts from 0 we add 1\n",
    "    D = X_train.shape[1] ## no of I/P features\n",
    "    W2 = 0.01*np.random.randn(h,K)    \n",
    "    b2 = np.zeros((1,K)) \n",
    "    W = 0.01*np.random.randn(D,h)\n",
    "    b = np.zeros((1,h))\n",
    "\n",
    "    #For simplicity we will take the batch size to be the same as number of examples\n",
    "    num_examples = X_train.shape[0]\n",
    "    \n",
    "    print(\"reg param:\",reg)\n",
    "    print(\"step size:\",step_size)\n",
    "    print(\"iterations:\",gd_iters)\n",
    "    \n",
    "    for i in range(gd_iters):\n",
    "        hidden_layer = np.maximum(0, np.dot(X_train, W) + b) \n",
    "        scores = np.dot(hidden_layer, W2) + b2\n",
    "        #print(hidden_layer.shape)\n",
    "\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores/np.sum(exp_scores, axis = 1, keepdims = True)\n",
    "\n",
    "        # compute the loss: average cross-entropy loss and regularization\n",
    "        corect_logprobs = -np.log(probs[range(num_examples),y_train])\n",
    "        data_loss = np.sum(corect_logprobs)/num_examples\n",
    "        reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n",
    "        loss = data_loss + reg_loss\n",
    "        #if i % 1000 == 0:\n",
    "            #print \"iteration %d: loss %f\" % (i, loss)\n",
    "\n",
    "        # compute the gradient on scores\n",
    "        dscores = probs\n",
    "        dscores[range(num_examples),y_train] -= 1\n",
    "        dscores /= num_examples\n",
    "\n",
    "        # backpropate the gradient to the parameters\n",
    "        dW2 = np.dot(hidden_layer.T, dscores)\n",
    "        db2 = np.sum(dscores, axis = 0, keepdims = True)       \n",
    "\n",
    "        # next backprop into hidden layer\n",
    "        dhidden = np.dot(dscores, W2.T)\n",
    "        dhidden[hidden_layer <= 0] =0   \n",
    "\n",
    "        ## final backprop\n",
    "        dW = np.dot(X_train.T, dhidden)\n",
    "        db = np.sum(dhidden, axis = 0, keepdims = True)\n",
    "        \n",
    "        ## adding reg to gradient\n",
    "        dW2 += reg*W2\n",
    "        dW += reg*W\n",
    "\n",
    "        ## stepsize\n",
    "        W += -step_size * dW\n",
    "        W2 += -step_size * dW2\n",
    "        b2 += -step_size * db2\n",
    "        b += -step_size * db\n",
    "    return W, b, W2, b2\n",
    "\n",
    "def test_ffn_classifier(X_data, y_data, W, b, W2, b2):\n",
    "    # Post-training: evaluate model accuracy\n",
    "    hidden_layer = np.maximum(0, np.dot(X_data, W) + b) \n",
    "    scores = np.dot(hidden_layer, W2) + b2\n",
    "    predicted_class = np.argmax(scores, axis = 1) ### scores with max conf\n",
    "    test_accuracy = (np.mean(predicted_class == y_data))\n",
    "    return test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split data into test and train with 80:20 ratio\n",
    "# Xtr, Xtst, ytr, ytst = test_train_data_split(X,y,0.8)\n",
    "\n",
    "W_tr, b_tr, W2_tr, b2_tr = train_ffn_classifier(Xtr, ytr, 0.4, 0.001, 100)\n",
    "\n",
    "# testing the performance on hold out set\n",
    "train_accuracy = test_ffn_classifier(Xtr, ytr, W_tr, b_tr, W2_tr, b2_tr)\n",
    "print(\"train accuracy: \",train_accuracy,)\n",
    "test_accuracy = test_ffn_classifier(Xtst, ytst, W_tr, b_tr, W2_tr, b2_tr)\n",
    "print(\"test accuracy: \",test_accuracy,\"\\n\")\n",
    "\n",
    "#plotting model result\n",
    "# plot_result(Xtst, ytst, W_tr, b_tr, W2_tr, b2_tr)\n",
    "\n",
    "#using several different values for the hyper parameter:\n",
    "step_size_values = list(np.arange(0,100,10))  #Also called learning rate\n",
    "# step_size_values = np.append(step_size_values, list(np.arange(1,10,1)))\n",
    "# step_size_values = np.append(step_size_values, list(np.arange(10,100,5)))\n",
    "\n",
    "test_results = [0] * len(step_size_values)\n",
    "print(\"Changing step size\")\n",
    "for i, step_size in enumerate(step_size_values):\n",
    "    W_tr, b_tr, W2_tr, b2_tr = train_ffn_classifier(Xtr, ytr, step_size, 0.001, 100)\n",
    "    test_result = test_ffn_classifier(Xtst, ytst, W_tr, b_tr, W2_tr, b2_tr)\n",
    "    print(\"test accuracy: \",test_result,\"\\n\")\n",
    "    test_results[i] = test_result\n",
    "\n",
    "plt.plot(step_size_values,test_results,'-')\n",
    "plt.axis([0, max(step_size_values)+1, 0, 1])\n",
    "plt.ylabel('test accuracy')\n",
    "plt.xlabel('learning rate')\n",
    "plt.show()\n",
    "\n",
    "# best value for regularization\n",
    "# we keep the learning rate to be constant\n",
    "# and alter the reg parameter\n",
    "reg_values = list([0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 1])\n",
    "\n",
    "test_results = [0] * len(reg_values)\n",
    "print(\"Changing reg parameter\")\n",
    "for i, reg_value in enumerate(reg_values):\n",
    "    W_tr, b_tr, W2_tr, b2_tr = train_ffn_classifier(Xtr, ytr, 0.4, reg_value, 100)\n",
    "    test_result = test_ffn_classifier(Xtst, ytst, W_tr, b_tr, W2_tr, b2_tr)\n",
    "    print(\"test accuracy: \",test_result,\"\\n\")\n",
    "    test_results[i] = test_result\n",
    "\n",
    "plt.plot(reg_values,test_results,'-')\n",
    "plt.axis([0, max(reg_values), 0, 1])\n",
    "plt.ylabel('test accuracy')\n",
    "plt.xlabel('regularization value')\n",
    "plt.show()\n",
    "\n",
    "# modifying the number of gradient descent iterations\n",
    "# reg = 0.4, step_size = 1\n",
    "gd_iters_vals = list([100, 500, 1000])\n",
    "\n",
    "test_results = [0] * len(gd_iters_vals)\n",
    "print(\"Changing no of iterations\")\n",
    "for i, gd_iters in enumerate(gd_iters_vals):\n",
    "    W_tr, b_tr, W2_tr, b2_tr = train_ffn_classifier(Xtr, ytr, 0.4, 0.001, gd_iters)\n",
    "    test_result = test_ffn_classifier(Xtst, ytst, W_tr, b_tr, W2_tr, b2_tr)\n",
    "    print(\"test accuracy: \",test_result,\"\\n\")\n",
    "    test_results[i] = test_result\n",
    "\n",
    "plt.plot(gd_iters_vals,test_results,'-')\n",
    "plt.axis([0, max(gd_iters_vals), 0, 1])\n",
    "plt.ylabel('test accuracy')\n",
    "plt.xlabel('no of iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##CIFAR 10 FFN Leaky RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_ffn_leakyrelu_classifier(X_train, y_train, step_size, reg, gd_iters):\n",
    "    h = 100 ## hidden layers\n",
    "    K = max(y_train)+1   ## no of O/P classes --> considerin class starts from 0 we add 1\n",
    "    D = X_train.shape[1] ## no of I/P features\n",
    "    W2 = 0.01*np.random.randn(h,K)  ## hidden layers *classes\n",
    "    b2 = np.zeros((1,K)) \n",
    "    W = 0.01*np.random.randn(D,h)\n",
    "    b = np.zeros((1,h))\n",
    "\n",
    "    #For simplicity we will take the batch size to be the same as number of examples\n",
    "    num_examples = X_train.shape[0]\n",
    "\n",
    "    print(\"reg param:\",reg)\n",
    "    print(\"step size:\",step_size)\n",
    "    print(\"iterations:\",gd_iters)\n",
    "    \n",
    "    for i in range(gd_iters):\n",
    "        hidden_layer = np.maximum(np.dot(X_train, W) + b, 0.01*(np.dot(X_train, W) + b))\n",
    "        scores = np.dot(hidden_layer, W2) + b2\n",
    "\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores/np.sum(exp_scores, axis = 1, keepdims = True)\n",
    "\n",
    "        # compute the loss: average cross-entropy loss and regularization\n",
    "        corect_logprobs = -np.log(probs[range(num_examples),y_train])\n",
    "        data_loss = np.sum(corect_logprobs)/num_examples\n",
    "        reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n",
    "        loss = data_loss + reg_loss\n",
    "        #if i % 1000 == 0:\n",
    "            #print \"iteration %d: loss %f\" % (i, loss)\n",
    "\n",
    "        # compute the gradient on scores\n",
    "        dscores = probs\n",
    "        dscores[range(num_examples),y_train] -= 1\n",
    "        dscores /= num_examples\n",
    "\n",
    "        # backpropate the gradient to the parameters\n",
    "        dW2 = np.dot(hidden_layer.T, dscores)\n",
    "        db2 = np.sum(dscores, axis = 0, keepdims = True)\n",
    "\n",
    "        # next backprop into hidden layer\n",
    "        dhidden = np.dot(dscores, W2.T)\n",
    "        var = 0.01*np.dot(dscores, W2.T)\n",
    "        \n",
    "#         for j in range(X_train.shape[0]):\n",
    "#             for k in range(X_train.shape[1]):\n",
    "#                 if dhidden[j,k] < 0:\n",
    "#                     dhidden[j,k] = var[j,k]\n",
    "        dhidden[hidden_layer <= 0] = 0.01*dhidden[hidden_layer <= 0]\n",
    "\n",
    "        ## final backprop\n",
    "        dW = np.dot(X_train.T, dhidden)\n",
    "        db = np.sum(dhidden, axis = 0, keepdims = True)\n",
    "\n",
    "        ## adding reg to gradient\n",
    "        dW2 += reg*W2\n",
    "        dW += reg*W\n",
    "\n",
    "        ## stepsize\n",
    "        W += -step_size * dW\n",
    "        W2 += -step_size * dW2\n",
    "        b2 += -step_size * db2\n",
    "        b += -step_size * db\n",
    "    return W, b, W2, b2\n",
    "\n",
    "def test_ffn_leakyrelu_classifier(X_data, y_data, W, b, W2, b2):\n",
    "    # Post-training: evaluate model accuracy\n",
    "    hidden_layer = np.maximum(0.01*np.dot(X_data, W) + b, np.dot(X_data, W) + b) \n",
    "    scores = np.dot(hidden_layer, W2) + b2\n",
    "    predicted_class = np.argmax(scores, axis = 1) ### scores with max conf\n",
    "    test_accuracy = (np.mean(predicted_class == y_data))\n",
    "    return test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split data into test and train with 80:20 ratio\n",
    "# Xtr, Xtst, ytr, ytst = test_train_data_split(X,y,0.8)\n",
    "\n",
    "# classifier trained using 5-fold cross validation\n",
    "W_tr, b_tr, W2_tr, b2_tr = train_ffn_leakyrelu_classifier(Xtr, ytr, 0.4, 0.001, 100)\n",
    "\n",
    "# testing the performance on hold out set\n",
    "train_accuracy = test_ffn_leakyrelu_classifier(Xtr, ytr, W_tr, b_tr, W2_tr, b2_tr)\n",
    "print(\"train accuracy: \",train_accuracy)\n",
    "test_accuracy = test_ffn_leakyrelu_classifier(Xtst, ytst, W_tr, b_tr, W2_tr, b2_tr)\n",
    "print(\"test accuracy: \",test_accuracy,\"\\n\")\n",
    "\n",
    "#plotting model result\n",
    "# plot_result(Xtst, ytst, W_tr, b_tr, W2_tr, b2_tr)\n",
    "\n",
    "#using several different values for the hyper parameter:\n",
    "step_size_values = list(np.arange(0,100,10))  #Also called learning rate\n",
    "# step_size_values = np.append(step_size_values, list(np.arange(1,10,1)))\n",
    "# step_size_values = np.append(step_size_values, list(np.arange(10,100,5)))\n",
    "\n",
    "test_results = [0] * len(step_size_values)\n",
    "print(\"Changing step size\")\n",
    "for i, step_size in enumerate(step_size_values):\n",
    "    W_tr, b_tr, W2_tr, b2_tr = train_ffn_leakyrelu_classifier(Xtr, ytr, step_size, 0.001, 100)\n",
    "    test_result = test_ffn_classifier(Xtst, ytst, W_tr, b_tr, W2_tr, b2_tr)\n",
    "    print(\"test accuracy: \",test_result,\"\\n\")\n",
    "    test_results[i] = test_result\n",
    "\n",
    "plt.plot(step_size_values,test_results,'-')\n",
    "plt.axis([0, max(step_size_values)+1, 0, 1])\n",
    "plt.ylabel('test accuracy')\n",
    "plt.xlabel('learning rate')\n",
    "plt.show()\n",
    "\n",
    "# best value for regularization\n",
    "# we keep the learning rate to be constant\n",
    "# and alter the reg parameter\n",
    "reg_values = list([0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 1])\n",
    "\n",
    "test_results = [0] * len(reg_values)\n",
    "print(\"Changing reg parameter\")\n",
    "for i, reg_value in enumerate(reg_values):\n",
    "    W_tr, b_tr, W2_tr, b2_tr = train_ffn_leakyrelu_classifier(Xtr, ytr, 0.4, reg_value, 100)\n",
    "    test_result = test_ffn_classifier(Xtst, ytst, W_tr, b_tr, W2_tr, b2_tr)\n",
    "    print(\"test accuracy: \",test_result,\"\\n\")\n",
    "    test_results[i] = test_result\n",
    "\n",
    "plt.plot(reg_values,test_results,'-')\n",
    "plt.axis([0, max(reg_values), 0, 1])\n",
    "plt.ylabel('test accuracy')\n",
    "plt.xlabel('regularization value')\n",
    "plt.show()\n",
    "\n",
    "# modifying the number of gradient descent iterations\n",
    "# reg = 0.4, step_size = 1\n",
    "# gd_iters_vals = list([100, 500, 1000])\n",
    "\n",
    "# test_results = [0] * len(gd_iters_vals)\n",
    "# print(\"Changing no of iterations\")\n",
    "# for i, gd_iters in enumerate(gd_iters_vals):\n",
    "#     W_tr, b_tr, W2_tr, b2_tr = train_ffn_leakyrelu_classifier(Xtr, ytr, 0.4, 0.8, gd_iters)\n",
    "#     test_result = test_ffn_classifier(Xtst, ytst, W_tr, b_tr, W2_tr, b2_tr)\n",
    "#     print(\"test accuracy: \",test_result,\"\\n\")\n",
    "#     test_results[i] = test_result\n",
    "\n",
    "# plt.plot(gd_iters_vals,test_results,'-')\n",
    "# plt.axis([0, max(gd_iters_vals), 0, 1])\n",
    "# plt.ylabel('test accuracy')\n",
    "# plt.xlabel('no of iterations')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##CIFAR 10 FFN Maxout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross validation scheme\n",
    "def cross_validate_maxout_classifier(Xtr, ytr, num_of_folds, step_size, reg, gd_iters):\n",
    "    # create stratified k folds of dataset for cross validation\n",
    "    skf = StratifiedKFold(ytr, n_folds=num_of_folds,random_state=0)\n",
    "    # store predicted accuracies of each fold\n",
    "    CV_pred_accuracies = []\n",
    "\n",
    "    for train_index, valid_index in skf:\n",
    "        X_train, X_valid = Xtr[train_index], Xtr[valid_index]\n",
    "        y_train, y_valid = ytr[train_index], ytr[valid_index]\n",
    "        W1, b1, W2, b2, W3, b3 = train_ffn_maxout_classifier(X_train, y_train, step_size, reg, gd_iters)\n",
    "        test_accuracy = test_ffn_maxout_classifier(X_valid, y_valid, W1, b1, W2, b2, W3, b3)\n",
    "        list.append(CV_pred_accuracies,test_accuracy)\n",
    "        print(\"cumulative CV accuracy: \", np.mean(CV_pred_accuracies),\"\\n\")\n",
    "    return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "# FFN model    \n",
    "def train_ffn_maxout_classifier(X_train, y_train, step_size, reg, gd_iters):\n",
    "    h = 100 ## hidden layers\n",
    "    K = max(y_train)+1   ## no of O/P classes --> considerin class starts from 0 we add 1\n",
    "    D = X_train.shape[1] ## no of I/P features\n",
    "    W3 = 0.01*np.random.randn(h,K)  ## hidden layers *classes\n",
    "    b3 = np.zeros((1,K)) \n",
    "    W1 = 0.01*np.random.randn(D,h)\n",
    "    b1 = np.zeros((1,h))\n",
    "    W2 = 0.01*np.random.randn(D,h)\n",
    "    b2 = np.zeros((1,h))\n",
    "\n",
    "    #For simplicity we will take the batch size to be the same as number of examples\n",
    "    num_examples = X_train.shape[0]\n",
    "    \n",
    "    print(\"reg param:\",reg)\n",
    "    print(\"step size:\",step_size)\n",
    "    print(\"iterations:\",gd_iters)\n",
    "    \n",
    "    for i in range(gd_iters):\n",
    "        hidden_layer1 = np.dot(X_train, W1) + b1\n",
    "        hidden_layer2 = np.dot(X_train, W2) + b2\n",
    "        hidden_layer = np.maximum(hidden_layer1, hidden_layer2)\n",
    "        scores = np.dot(hidden_layer, W3) + b3\n",
    "#         scores2 = np.dot(hidden_layer2, W3) + b3\n",
    "\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores/np.sum(exp_scores, axis = 1, keepdims = True)\n",
    "        \n",
    "#         exp_scores2 = np.exp(scores2)\n",
    "#         probs2 = exp_scores2/np.sum(exp_scores2, axis = 1, keepdims = True)        \n",
    "        \n",
    "#         scores = scores1 + scores2\n",
    "#         exp_scores = np.exp(scores)\n",
    "#         probs = exp_scores/np.sum(exp_scores, axis = 1, keepdims = True)\n",
    "        \n",
    "        \n",
    "        # compute the loss: average cross-entropy loss and regularization\n",
    "        corect_logprobs = -np.log(probs[range(num_examples),y_train])\n",
    "        data_loss = np.sum(corect_logprobs)/num_examples\n",
    "        reg_loss = 0.33*reg*np.sum(W1*W1) + 0.33*reg*np.sum(W2*W2) + 0.34*reg*np.sum(W3*W3)\n",
    "        loss = data_loss + reg_loss\n",
    "        #if i % 1000 == 0:\n",
    "            #print \"iteration %d: loss %f\" % (i, loss)\n",
    "\n",
    "        # compute the gradient on scores\n",
    "#         dscores1 = probs1\n",
    "#         dscores1[range(num_examples),y] -= 1\n",
    "#         dscores1 /= num_examples\n",
    "        \n",
    "#         dscores2 = probs2\n",
    "#         dscores2[range(num_examples),y] -= 1\n",
    "#         dscores2 /= num_examples       \n",
    "        \n",
    "        dscores = probs\n",
    "        dscores[range(num_examples),y_train] -= 1\n",
    "        dscores /= num_examples\n",
    "\n",
    "        # backpropate the gradient to the parameters\n",
    "        dW3 = np.dot(hidden_layer.T, dscores)\n",
    "        db3 = np.sum(dscores, axis = 0, keepdims = True)\n",
    "\n",
    "        # next backprop into hidden layer\n",
    "        dhidden1 = np.dot(dscores, W3.T)\n",
    "        dhidden1[hidden_layer1 <= hidden_layer2] = 0\n",
    "        dhidden2 = np.dot(dscores, W3.T)\n",
    "        dhidden2[hidden_layer2 <= hidden_layer1] = 0\n",
    "\n",
    "#         for j in range(hidden_layer2.shape[0]):\n",
    "#             for k in range(hidden_layer2.shape[1]):\n",
    "#                 if hidden_layer1[j,k]<hidden_layer2[j,k]:\n",
    "#                     dhidden1[j,k] = 0\n",
    "#                 elif hidden_layer2[j,k]<=hidden_layer1[j,k]:\n",
    "#                     dhidden2[j,k] = 0\n",
    "        \n",
    "        ## final backprop\n",
    "        dW1 = np.dot(X_train.T, dhidden1)\n",
    "        db1 = np.sum(dhidden1, axis = 0, keepdims = True)\n",
    "\n",
    "        dW2 = np.dot(X_train.T, dhidden2)\n",
    "        db2 = np.sum(dhidden2, axis = 0, keepdims = True)\n",
    "        ## adding reg to gradient\n",
    "        dW2 += reg*W2\n",
    "        dW1 += reg*W1\n",
    "        dW3 += reg*W3\n",
    "\n",
    "        ## stepsize\n",
    "        W1 += -step_size * dW1\n",
    "        W2 += -step_size * dW2\n",
    "        W3 += -step_size * dW3\n",
    "        \n",
    "        b2 += -step_size * db2\n",
    "        b1 += -step_size * db1\n",
    "        b3 += -step_size * db3\n",
    "    return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "def test_ffn_maxout_classifier(X_data, y_data, W1, b1, W2, b2, W3, b3):\n",
    "    # Post-training: evaluate model accuracy\n",
    "    hidden_layer = np.maximum(np.dot(X_data, W1) + b1, np.dot(X_data, W2) + b2) \n",
    "    scores = np.dot(hidden_layer, W3) + b3\n",
    "    predicted_class = np.argmax(scores, axis = 1) ### scores with max conf\n",
    "    test_accuracy = (np.mean(predicted_class == y_data))\n",
    "    return test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split data into test and train with 80:20 ratio\n",
    "# Xtr, Xtst, ytr, ytst = test_train_data_split(X, y, 0.8)\n",
    "\n",
    "# classifier trained using 5-fold cross validation\n",
    "W1_tr, b1_tr, W2_tr, b2_tr, W3_tr, b3_tr = train_ffn_maxout_classifier(Xtr, ytr, 0.4, 0.001, 100)\n",
    "\n",
    "# testing the performance on hold out set\n",
    "train_accuracy = test_ffn_maxout_classifier(Xtr, ytr, W1_tr, b1_tr, W2_tr, b2_tr, W3_tr, b3_tr)\n",
    "print(\"train accuracy: \",train_accuracy)\n",
    "test_accuracy = test_ffn_maxout_classifier(Xtst, ytst, W1_tr, b1_tr, W2_tr, b2_tr, W3_tr, b3_tr)\n",
    "print(\"test accuracy: \",test_accuracy,\"\\n\")\n",
    "\n",
    "#plotting model result\n",
    "# plot_result_for_maxout(Xtst, ytst, W1_tr, b1_tr, W2_tr, b2_tr, W3_tr, b3_tr)\n",
    "\n",
    "#using several different values for the hyper parameter:\n",
    "step_size_values = list(np.arange(0,100,10))  #Also called learning rate\n",
    "# step_size_values = np.append(step_size_values, list(np.arange(1,10,1)))\n",
    "# step_size_values = np.append(step_size_values, list(np.arange(10,100,5)))\n",
    "\n",
    "test_results = [0] * len(step_size_values)\n",
    "\n",
    "for i, step_size in enumerate(step_size_values):\n",
    "    W1_tr, b1_tr, W2_tr, b2_tr, W3_tr, b3_tr = train_ffn_maxout_classifier(Xtr, ytr, step_size, 0.001, 100)\n",
    "    test_result = test_ffn_maxout_classifier(Xtst, ytst, W1_tr, b1_tr, W2_tr, b2_tr, W3_tr, b3_tr)\n",
    "    print(\"test accuracy: \",test_result,\"\\n\")\n",
    "    test_results[i] = test_result\n",
    "\n",
    "plt.plot(step_size_values,test_results,'-')\n",
    "plt.axis([0, max(step_size_values)+1, 0, 1])\n",
    "plt.ylabel('test accuracy')\n",
    "plt.xlabel('learning rate')\n",
    "plt.show()\n",
    "\n",
    "# best value for regularization\n",
    "# we keep the learning rate to be constant\n",
    "# and alter the reg parameter\n",
    "reg_values = list([0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 1])\n",
    "\n",
    "test_results = [0] * len(reg_values)\n",
    "\n",
    "for i, reg_value in enumerate(reg_values):\n",
    "    W1_tr, b1_tr, W2_tr, b2_tr, W3_tr, b3_tr = train_ffn_maxout_classifier(Xtr, ytr, 0.4, reg_value, 100)\n",
    "    test_result = test_ffn_maxout_classifier(Xtst, ytst, W1_tr, b1_tr, W2_tr, b2_tr, W3_tr, b3_tr)\n",
    "    print(\"test accuracy: \",test_result,\"\\n\")\n",
    "    test_results[i] = test_result\n",
    "\n",
    "plt.plot(reg_values,test_results,'-')\n",
    "plt.axis([0, max(reg_values), 0, 1])\n",
    "plt.ylabel('test accuracy')\n",
    "plt.xlabel('regularization value')\n",
    "plt.show()\n",
    "\n",
    "# modifying the number of gradient descent iterations\n",
    "# reg = 0.4, step_size = 1\n",
    "# gd_iters_vals = list([100, 500, 1000])\n",
    "\n",
    "# test_results = [0] * len(gd_iters_vals)\n",
    "\n",
    "# for i, gd_iters in enumerate(gd_iters_vals):\n",
    "#     W1_tr, b1_tr, W2_tr, b2_tr, W3_tr, b3_tr = train_ffn_maxout_classifier(Xtr, ytr, 0.4, 0.8, gd_iters)\n",
    "#     test_result = test_ffn_maxout_classifier(Xtst, ytst, W1_tr, b1_tr, W2_tr, b2_tr, W3_tr, b3_tr)\n",
    "#     print(\"test accuracy: \",test_result,\"\\n\")\n",
    "#     test_results[i] = test_result\n",
    "\n",
    "# plt.plot(gd_iters_vals,test_results,'-')\n",
    "# plt.axis([0, max(gd_iters_vals), 0, 1])\n",
    "# plt.ylabel('test accuracy')\n",
    "# plt.xlabel('no of iterations')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
