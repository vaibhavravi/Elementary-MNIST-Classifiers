{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Imports \n",
    "import numpy as np #Represent ndarrays a.k.a. tensors\n",
    "import matplotlib.pyplot as plt #For plotting\n",
    "np.random.seed(0) #For repeatability of the experiment\n",
    "import pickle #To read data for this experiment\n",
    "import _pickle as cPickle\n",
    "import gzip\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "#Setup\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Read data\n",
    "def load_data():\n",
    "    with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "        training_data, validation_data, test_data = cPickle.load(f, encoding=\"latin-1\")\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = np.array(list(zip(training_inputs, training_results)))\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = np.array(list(zip(validation_inputs, va_d[1])))\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = np.array(list(zip(test_inputs, te_d[1])))\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "training_data, validation_data, test_data = load_data_wrapper()\n",
    "\n",
    "training_data_nolab = np.delete(training_data,[1],axis = 1)\n",
    "training_labels = np.array(np.delete(training_data,[0],axis=1))\n",
    "training_labels_list = []\n",
    "training_data_nolab_list = np.zeros(shape=(50000,784))\n",
    "\n",
    "for i in range(len(training_labels)):\n",
    "    list.append(training_labels_list, np.argmax(training_labels[i,0]))\n",
    "\n",
    "for i in range(len(training_data_nolab)):\n",
    "    training_data_nolab_list[i] = np.array(np.ravel(training_data_nolab[i,0]))\n",
    "\n",
    "Xtr = training_data_nolab_list\n",
    "ytr = np.squeeze(training_labels_list)\n",
    "\n",
    "test_data_nolab = np.delete(test_data,[1],axis = 1)\n",
    "test_labels = np.array(np.delete(test_data,[0],axis=1))\n",
    "test_labels_list = []\n",
    "test_data_nolab_list = np.zeros(shape=(10000,784))\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    list.append(test_labels_list, np.argmax(test_labels[i,0]))\n",
    "\n",
    "for i in range(len(test_data_nolab)):\n",
    "    test_data_nolab_list[i] = np.array(np.ravel(test_data_nolab[i,0]))\n",
    "\n",
    "Xtst = test_data_nolab_list\n",
    "ytst = np.squeeze(test_labels_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cross validation scheme\n",
    "def cross_validate_classifier(Xtr, ytr, num_of_folds):\n",
    "    # create stratified k folds of dataset for cross validation\n",
    "    skf = StratifiedKFold(ytr, n_folds=num_of_folds,random_state=0)\n",
    "    # store predicted accuracies of each fold\n",
    "    CV_pred_accuracies = []\n",
    "\n",
    "    for train_index, valid_index in skf:\n",
    "        X_train, X_valid = Xtr[train_index], Xtr[valid_index]\n",
    "        y_train, y_valid = ytr[train_index], ytr[valid_index]\n",
    "        W, b = train_linear_classifier(X_train, y_train, 8e-1, 2e-3, 100)\n",
    "        test_accuracy = test_linear_classifier(X_valid, y_valid, W, b)\n",
    "        list.append(CV_pred_accuracies,test_accuracy)\n",
    "        print(\"cumulative CV accuracy: \", np.mean(CV_pred_accuracies),\"\\n\")\n",
    "    return W, b\n",
    "\n",
    "# Linear model    \n",
    "def train_linear_classifier(X_train, y_train, step_size, reg, gd_iters):    \n",
    "    #Define some local varaibles\n",
    "    D = X.shape[1] #Number of features 28x28 = 784 for mnist\n",
    "    K = 10 #Number of classes assuming class index starts from 0\n",
    "\n",
    "    # Start with an initialize parameters randomly\n",
    "    W = 0.01 * np.random.randn(D,K)\n",
    "    b = np.zeros((1,K))\n",
    "\n",
    "    #For simplicity we will take the batch size to be the same as number of examples\n",
    "    num_examples = X.shape[0]\n",
    "    \n",
    "    print(\"reg param:\",reg)\n",
    "    print(\"step size:\",step_size)\n",
    "    print(\"iterations:\",gd_iters)\n",
    "    \n",
    "    # gradient descent loop\n",
    "    for i in range(gd_iters):\n",
    "\n",
    "        # evaluate class scores, [N x K]\n",
    "        scores = np.dot(X, W) + b\n",
    "        #print(\"score values\")\n",
    "        #print(scores[:10])\n",
    "        #print(\"length:\",len(scores))\n",
    "\n",
    "        # compute the class probabilities\n",
    "        exp_scores = np.exp(scores)\n",
    "        # print(\"score values\")\n",
    "        # print(exp_scores[:10])\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "\n",
    "        # compute the loss: average cross-entropy loss and regularization\n",
    "        corect_logprobs = -np.log(probs[range(num_examples),y])\n",
    "        data_loss = np.sum(corect_logprobs)/num_examples\n",
    "        reg_loss = 0.5*reg*np.sum(W*W)\n",
    "        loss = data_loss + reg_loss\n",
    "        if i % 50 == 0:\n",
    "            print(\"iteration:\",i, \" loss:\",loss)\n",
    "\n",
    "        # compute the gradient on scores\n",
    "        dscores = probs\n",
    "        dscores[range(num_examples),y] -= 1\n",
    "        dscores /= num_examples\n",
    "\n",
    "        # backpropate the gradient to the parameters (W,b)\n",
    "        dW = np.dot(X.T, dscores)\n",
    "        db = np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "        dW += reg*W # regularization gradient\n",
    "\n",
    "        # perform a parameter update\n",
    "        W += -step_size * dW\n",
    "        b += -step_size * db\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_linear_classifier(X_data, y_data, W, b):\n",
    "    # Post-training: evaluate validation set accuracy\n",
    "    scores = np.dot(X_data, W) + b\n",
    "    predicted_class = np.argmax(scores, axis=1)\n",
    "    test_accuracy = (np.mean(predicted_class == y_data))\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Post Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg param: 0.002\n",
      "step size: 0.8\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.3212349842\n",
      "iteration: 50  loss: 0.452181966512\n",
      "cumulative CV accuracy:  0.905347326337 \n",
      "\n",
      "reg param: 0.002\n",
      "step size: 0.8\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.31259563997\n",
      "iteration: 50  loss: 0.452227981408\n",
      "cumulative CV accuracy:  0.901983801141 \n",
      "\n",
      "reg param: 0.002\n",
      "step size: 0.8\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.3074130784\n",
      "iteration: 50  loss: 0.452242550683\n",
      "cumulative CV accuracy:  0.901689200761 \n",
      "\n",
      "reg param: 0.002\n",
      "step size: 0.8\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.31483938144\n",
      "iteration: 50  loss: 0.452032424269\n",
      "cumulative CV accuracy:  0.899533880664 \n",
      "\n",
      "reg param: 0.002\n",
      "step size: 0.8\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.31109355176\n",
      "iteration: 50  loss: 0.452306275297\n",
      "cumulative CV accuracy:  0.898958837225 \n",
      "\n",
      "test accuracy:  0.1028 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 0.0\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.30630667265\n",
      "iteration: 50  loss: 2.30630667265\n",
      "test accuracy:  0.4162 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 0.1\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.32615504715\n",
      "iteration: 50  loss: 0.813774258013\n",
      "test accuracy:  0.1042 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 0.2\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.31865508653\n",
      "iteration: 50  loss: 0.625305643567\n",
      "test accuracy:  0.1037 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 0.3\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.30620223715\n",
      "iteration: 50  loss: 0.549775533651\n",
      "test accuracy:  0.1033 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 0.4\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.32294992797\n",
      "iteration: 50  loss: 0.50844619306\n",
      "test accuracy:  0.1035 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 0.5\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.29946493988\n",
      "iteration: 50  loss: 0.480898737738\n",
      "test accuracy:  0.1031 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 0.6\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.32390517137\n",
      "iteration: 50  loss: 0.461345718276\n",
      "test accuracy:  0.1028 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 0.7\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.31284438353\n",
      "iteration: 50  loss: 0.44581645125\n",
      "test accuracy:  0.1027 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 0.8\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.3060470922\n",
      "iteration: 50  loss: 0.433547033993\n",
      "test accuracy:  0.1026 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 0.9\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.30790131077\n",
      "iteration: 50  loss: 0.422952889906\n",
      "test accuracy:  0.1025 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 1.0\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.31562595976\n",
      "iteration: 50  loss: 0.414403408472\n",
      "test accuracy:  0.1022 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 2.0\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.3026341579\n",
      "iteration: 50  loss: 0.475109928265\n",
      "test accuracy:  0.0972 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 3.0\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.2882225492\n",
      "iteration: 50  loss: 1.04519142182\n",
      "test accuracy:  0.0986 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 4.0\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.30254659878\n",
      "iteration: 50  loss: 1.20516982883\n",
      "test accuracy:  0.0959 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 5.0\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.30656920718\n",
      "iteration: 50  loss: 3.29431464526\n",
      "test accuracy:  0.0257 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 6.0\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.31236747103\n",
      "iteration: 50  loss: 1.0460990263\n",
      "test accuracy:  0.0843 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 7.0\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.3238570975\n",
      "iteration: 50  loss: 4.48649597395\n",
      "test accuracy:  0.1042 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 8.0\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.29537079882\n",
      "iteration: 50  loss: 2.4805096796\n",
      "test accuracy:  0.1142 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 9.0\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.29345397647\n",
      "iteration: 50  loss: 3.50093523637\n",
      "test accuracy:  0.1029 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 10.0\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.32337777838\n",
      "iteration: 50  loss: 5.7734822198\n",
      "test accuracy:  0.0997 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 15.0\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.30717576052\n",
      "iteration: 50  loss: 12.3946305699\n",
      "test accuracy:  0.1065 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 20.0\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.30558697274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ybow_93/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:51: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 50  loss: 10.1716859914\n",
      "test accuracy:  0.0926 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 25.0\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.28446734615\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-ece2c848f373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_size_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mW_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_linear_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mtest_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_linear_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-ac8805ab64ec>\u001b[0m in \u001b[0;36mtrain_linear_classifier\u001b[0;34m(X_train, y_train, step_size, reg, gd_iters)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# compute the gradient on scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mdscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mdscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mdscores\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mnum_examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# classifier trained using 5-fold cross validation\n",
    "W_tr, b_tr = cross_validate_classifier(Xtr, ytr, 5)\n",
    "\n",
    "# testing the performance on hold out set\n",
    "test_accuracy = test_linear_classifier(Xtst, ytst, W_tr, b_tr)\n",
    "print(\"test accuracy: \",test_accuracy,\"\\n\")\n",
    "\n",
    "#plotting model result\n",
    "#plot_result(Xtst, ytst, W_tr, b_tr)\n",
    "\n",
    "#using several different values for the hyper parameter:\n",
    "step_size_values = list(np.arange(0,1,0.1))  #Also called learning rate\n",
    "step_size_values = np.append(step_size_values, list(np.arange(1,10,1)))\n",
    "\n",
    "test_results = [0] * len(step_size_values)\n",
    "\n",
    "for i, step_size in enumerate(step_size_values):\n",
    "    W_tr, b_tr = train_linear_classifier(Xtr, ytr, step_size, 1e-3, 100)\n",
    "    test_result = test_linear_classifier(Xtst, ytst, W_tr, b_tr)\n",
    "    print(\"test accuracy: \",test_result,\"\\n\")\n",
    "    test_results[i] = test_result\n",
    "\n",
    "plt.plot(step_size_values,test_results,'-')\n",
    "plt.axis([0, max(step_size_values)+1, 0, 1])\n",
    "plt.ylabel('test accuracy')\n",
    "plt.xlabel('learning rate')\n",
    "plt.show()\n",
    "\n",
    "# best value for regularization\n",
    "# we keep the learning rate to be constant\n",
    "# and alter the reg parameter\n",
    "reg_values = list([0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1])\n",
    "\n",
    "test_results = [0] * len(reg_values)\n",
    "\n",
    "for i, reg_value in enumerate(reg_values):\n",
    "    W_tr, b_tr = train_linear_classifier(Xtr, ytr, 1, reg_value, 100)\n",
    "    test_result = test_linear_classifier(Xtst, ytst, W_tr, b_tr)\n",
    "    print(\"test accuracy: \",test_result,\"\\n\")\n",
    "    test_results[i] = test_result\n",
    "\n",
    "plt.plot(reg_values,test_results,'-')\n",
    "plt.axis([0, max(reg_values), 0, 1])\n",
    "plt.ylabel('test accuracy')\n",
    "plt.xlabel('regularization value')\n",
    "plt.show()\n",
    "\n",
    "# modifying the number of gradient descent iterations\n",
    "# reg = 0.4, step_size = 1\n",
    "gd_iters_vals = list([100, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "test_results = [0] * len(gd_iters_vals)\n",
    "\n",
    "for i, gd_iters in enumerate(gd_iters_vals):\n",
    "    W_tr, b_tr = train_linear_classifier(Xtr, ytr, 1, 0.4, gd_iters)\n",
    "    test_result = test_linear_classifier(Xtst, ytst, W_tr, b_tr)\n",
    "    print(\"test accuracy: \",test_result,\"\\n\")\n",
    "    test_results[i] = test_result\n",
    "\n",
    "plt.plot(gd_iters_vals,test_results,'-')\n",
    "plt.axis([0, max(gd_iters_vals), 0, 1])\n",
    "plt.ylabel('test accuracy')\n",
    "plt.xlabel('no of iterations')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg param: 0.001\n",
      "step size: 0.0\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.31133163909\n",
      "iteration: 50  loss: 2.31133163909\n",
      "test accuracy:  0.0263 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 0.1\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.32093909799\n",
      "iteration: 50  loss: 0.815655393825\n",
      "test accuracy:  0.1043 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 0.2\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.30834819475\n",
      "iteration: 50  loss: 0.62486140807\n",
      "test accuracy:  0.1034 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 0.3\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.31290299124\n",
      "iteration: 50  loss: 0.549981015521\n",
      "test accuracy:  0.1034 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 0.4\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.29644878086\n",
      "iteration: 50  loss: 0.508139334811\n",
      "test accuracy:  0.1037 \n",
      "\n",
      "reg param: 0.001\n",
      "step size: 0.5\n",
      "iterations: 100\n",
      "iteration: 0  loss: 2.30281638713\n"
     ]
    }
   ],
   "source": [
    "#using several different values for the hyper parameter:\n",
    "step_size_values = list(np.arange(0,1,0.1))  #Also called learning rate\n",
    "step_size_values = np.append(step_size_values, list(np.arange(1,10,1)))\n",
    "\n",
    "test_results = [0] * len(step_size_values)\n",
    "\n",
    "for i, step_size in enumerate(step_size_values):\n",
    "    W_tr, b_tr = train_linear_classifier(Xtr, ytr, step_size, 1e-3, 100)\n",
    "    test_result = test_linear_classifier(Xtst, ytst, W_tr, b_tr)\n",
    "    print(\"test accuracy: \",test_result,\"\\n\")\n",
    "    test_results[i] = test_result\n",
    "\n",
    "plt.plot(step_size_values,test_results,'-')\n",
    "plt.axis([0, max(step_size_values)+1, 0, 1])\n",
    "plt.ylabel('test accuracy')\n",
    "plt.xlabel('learning rate')\n",
    "plt.show()\n",
    "\n",
    "# best value for regularization\n",
    "# we keep the learning rate to be constant\n",
    "# and alter the reg parameter\n",
    "reg_values = list([0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1])\n",
    "\n",
    "test_results = [0] * len(reg_values)\n",
    "\n",
    "for i, reg_value in enumerate(reg_values):\n",
    "    W_tr, b_tr = train_linear_classifier(Xtr, ytr, 1, reg_value, 100)\n",
    "    test_result = test_linear_classifier(Xtst, ytst, W_tr, b_tr)\n",
    "    print(\"test accuracy: \",test_result,\"\\n\")\n",
    "    test_results[i] = test_result\n",
    "\n",
    "plt.plot(reg_values,test_results,'-')\n",
    "plt.axis([0, max(reg_values), 0, 1])\n",
    "plt.ylabel('test accuracy')\n",
    "plt.xlabel('regularization value')\n",
    "plt.show()\n",
    "\n",
    "# modifying the number of gradient descent iterations\n",
    "# reg = 0.4, step_size = 1\n",
    "gd_iters_vals = list([100, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "test_results = [0] * len(gd_iters_vals)\n",
    "\n",
    "for i, gd_iters in enumerate(gd_iters_vals):\n",
    "    W_tr, b_tr = train_linear_classifier(Xtr, ytr, 1, 0.4, gd_iters)\n",
    "    test_result = test_linear_classifier(Xtst, ytst, W_tr, b_tr)\n",
    "    print(\"test accuracy: \",test_result,\"\\n\")\n",
    "    test_results[i] = test_result\n",
    "\n",
    "plt.plot(gd_iters_vals,test_results,'-')\n",
    "plt.axis([0, max(gd_iters_vals), 0, 1])\n",
    "plt.ylabel('test accuracy')\n",
    "plt.xlabel('no of iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
